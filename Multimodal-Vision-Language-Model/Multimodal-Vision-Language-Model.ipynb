{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9883671,"sourceType":"datasetVersion","datasetId":6069100},{"sourceId":9894530,"sourceType":"datasetVersion","datasetId":6077265},{"sourceId":9894758,"sourceType":"datasetVersion","datasetId":6077449},{"sourceId":9895923,"sourceType":"datasetVersion","datasetId":6078339}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[{"file_id":"https://storage.googleapis.com/kaggle-colab-exported-notebooks/multimodal-vision-language-model-975a1bbc-227f-48d3-98b1-7f66d7def518.ipynb?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com/20241107/auto/storage/goog4_request&X-Goog-Date=20241107T134840Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=8524545b094886662c7c836af716a0136ee703d9696af5d220107d7540c926ba1260349048858f4088e489867dbbfabd6b8ff2d63c487435cfd2c1a90bb93d0ed4c0e3f5664774f534b2300b6e277099affb7ba7f814a9a04a39ec1204e491094f2e849477e893b6a7e74fd5fd2646ae2ccf621f4eddf21a8f9f76c6859b921654a521053b4ca17e28528c33a16fb282992ced9b3c04f6cf645c409211a6cda7be1b0fd6d65839ccd2eade8f81dad3710995a0b8e3019a1b1a29a33613559103e09fbca574d4af7a78577f7496a55c3936e50a2e2cb330bdcc93685794e04d3caf29701a7db51228639fb2b6bd01ae5bed780e60456baff2972eb4e261a87e4d","timestamp":1731067091956}],"gpuType":"T4"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Requirements","metadata":{}},{"cell_type":"code","source":"%%writefile requirements.txt\n\nfire==0.6.0\n\nnumpy==1.26.4\n\npillow==10.3.0\n\nsafetensors==0.4.3\n\ntokenizers==0.19.1\n\ntorch==2.3.0\n\ntorchaudio==2.3.0\n\ntorchvision==0.18.0\n\ntqdm==4.66.4\n\ntransformers==4.41.2","metadata":{"_uuid":"cecd5f23-5881-4a8f-91a4-3036d065f97c","_cell_guid":"9e984f9a-f0b1-4269-abfd-7773dee4241e","trusted":true,"id":"9B-4UQocCa3W","executionInfo":{"status":"ok","timestamp":1730990301331,"user_tz":-330,"elapsed":487,"user":{"displayName":"","userId":""}},"outputId":"9f1cfaaf-e3a1-4fc7-b620-cebcdac73399","execution":{"iopub.status.busy":"2024-11-16T11:59:46.148059Z","iopub.execute_input":"2024-11-16T11:59:46.148454Z","iopub.status.idle":"2024-11-16T11:59:46.154365Z","shell.execute_reply.started":"2024-11-16T11:59:46.148417Z","shell.execute_reply":"2024-11-16T11:59:46.153351Z"}},"outputs":[{"name":"stdout","text":"Overwriting requirements.txt\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip install -r requirements.txt","metadata":{"_uuid":"94b413b9-c527-4d34-9477-27803fe08e03","_cell_guid":"0a948a8c-b2ca-4022-9f91-f6d74744a913","trusted":true,"id":"qiMlY-4FChIJ","executionInfo":{"status":"ok","timestamp":1730990247284,"user_tz":-330,"elapsed":222314,"user":{"displayName":"","userId":""}},"outputId":"9d117ad6-292c-4cbd-8dcf-4f1364fdc514","execution":{"iopub.status.busy":"2024-11-16T11:59:48.289063Z","iopub.execute_input":"2024-11-16T11:59:48.289912Z","iopub.status.idle":"2024-11-16T12:02:31.391195Z","shell.execute_reply.started":"2024-11-16T11:59:48.289873Z","shell.execute_reply":"2024-11-16T12:02:31.389765Z"}},"outputs":[{"name":"stdout","text":"Collecting fire==0.6.0 (from -r requirements.txt (line 2))\n  Downloading fire-0.6.0.tar.gz (88 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy==1.26.4 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (1.26.4)\nRequirement already satisfied: pillow==10.3.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (10.3.0)\nCollecting safetensors==0.4.3 (from -r requirements.txt (line 8))\n  Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\nCollecting tokenizers==0.19.1 (from -r requirements.txt (line 10))\n  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nCollecting torch==2.3.0 (from -r requirements.txt (line 12))\n  Downloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\nCollecting torchaudio==2.3.0 (from -r requirements.txt (line 14))\n  Downloading torchaudio-2.3.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\nCollecting torchvision==0.18.0 (from -r requirements.txt (line 16))\n  Downloading torchvision-0.18.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: tqdm==4.66.4 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 18)) (4.66.4)\nCollecting transformers==4.41.2 (from -r requirements.txt (line 20))\n  Downloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire==0.6.0->-r requirements.txt (line 2)) (1.16.0)\nRequirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from fire==0.6.0->-r requirements.txt (line 2)) (2.4.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from tokenizers==0.19.1->-r requirements.txt (line 10)) (0.25.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0->-r requirements.txt (line 12)) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0->-r requirements.txt (line 12)) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0->-r requirements.txt (line 12)) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0->-r requirements.txt (line 12)) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0->-r requirements.txt (line 12)) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0->-r requirements.txt (line 12)) (2024.6.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.0->-r requirements.txt (line 12))\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.0->-r requirements.txt (line 12))\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.0->-r requirements.txt (line 12))\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.0->-r requirements.txt (line 12))\n  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.0->-r requirements.txt (line 12))\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.0->-r requirements.txt (line 12))\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.0->-r requirements.txt (line 12))\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.0->-r requirements.txt (line 12))\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.0->-r requirements.txt (line 12))\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0->-r requirements.txt (line 12))\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.0->-r requirements.txt (line 12))\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==2.3.0 (from torch==2.3.0->-r requirements.txt (line 12))\n  Downloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.41.2->-r requirements.txt (line 20)) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.41.2->-r requirements.txt (line 20)) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.41.2->-r requirements.txt (line 20)) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.41.2->-r requirements.txt (line 20)) (2.32.3)\nCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0->-r requirements.txt (line 12))\n  Downloading nvidia_nvjitlink_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.41.2->-r requirements.txt (line 20)) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.3.0->-r requirements.txt (line 12)) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.41.2->-r requirements.txt (line 20)) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.41.2->-r requirements.txt (line 20)) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.41.2->-r requirements.txt (line 20)) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.41.2->-r requirements.txt (line 20)) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.3.0->-r requirements.txt (line 12)) (1.3.0)\nDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torchaudio-2.3.0-cp310-cp310-manylinux1_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading torchvision-0.18.0-cp310-cp310-manylinux1_x86_64.whl (7.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading transformers-4.41.2-py3-none-any.whl (9.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m99.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: fire\n  Building wheel for fire (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117029 sha256=ffa0c82d4f98448f42c10dd32f464e4bac1736acc9a81fc5dcfd157f9b8cf0aa\n  Stored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\nSuccessfully built fire\nInstalling collected packages: triton, safetensors, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fire, nvidia-cusparse-cu12, nvidia-cudnn-cu12, tokenizers, nvidia-cusolver-cu12, transformers, torch, torchvision, torchaudio\n  Attempting uninstall: safetensors\n    Found existing installation: safetensors 0.4.5\n    Uninstalling safetensors-0.4.5:\n      Successfully uninstalled safetensors-0.4.5\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.20.0\n    Uninstalling tokenizers-0.20.0:\n      Successfully uninstalled tokenizers-0.20.0\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.45.1\n    Uninstalling transformers-4.45.1:\n      Successfully uninstalled transformers-4.45.1\n  Attempting uninstall: torch\n    Found existing installation: torch 2.4.0\n    Uninstalling torch-2.4.0:\n      Successfully uninstalled torch-2.4.0\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.19.0\n    Uninstalling torchvision-0.19.0:\n      Successfully uninstalled torchvision-0.19.0\n  Attempting uninstall: torchaudio\n    Found existing installation: torchaudio 2.4.0\n    Uninstalling torchaudio-2.4.0:\n      Successfully uninstalled torchaudio-2.4.0\nSuccessfully installed fire-0.6.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.77 nvidia-nvtx-cu12-12.1.105 safetensors-0.4.3 tokenizers-0.19.1 torch-2.3.0 torchaudio-2.3.0 torchvision-0.18.0 transformers-4.41.2 triton-2.3.0\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Modeling_Siglip","metadata":{}},{"cell_type":"code","source":"%%writefile modeling_siglip.py\nfrom typing import Optional, Tuple\nimport torch\nimport torch.nn as nn\n\nclass SiglipVisionConfig:\n\n    def __init__(\n        self,\n        hidden_size=768,\n        intermediate_size=3072,\n        num_hidden_layers=12,\n        num_attention_heads=12,\n        num_channels=3,\n        image_size=224,\n        patch_size=16,\n        layer_norm_eps=1e-6,\n        attention_dropout=0.0,\n        num_image_tokens: int = None,\n        **kwargs\n    ):\n        super().__init__()\n\n        self.hidden_size = hidden_size\n        self.intermediate_size = intermediate_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.num_channels = num_channels\n        self.patch_size = patch_size\n        self.image_size = image_size\n        self.attention_dropout = attention_dropout\n        self.layer_norm_eps = layer_norm_eps\n        self.num_image_tokens = num_image_tokens\n\n\nclass SiglipVisionEmbeddings(nn.Module):\n    def __init__(self, config: SiglipVisionConfig):\n        super().__init__()\n        self.config = config\n        self.embed_dim = config.hidden_size\n        self.image_size = config.image_size\n        self.patch_size = config.patch_size\n\n        self.patch_embedding = nn.Conv2d(\n            in_channels=config.num_channels,\n            out_channels=self.embed_dim,\n            kernel_size=self.patch_size,\n            stride=self.patch_size,\n            padding=\"valid\", # This indicates no padding is added\n        )\n\n        self.num_patches = (self.image_size // self.patch_size) ** 2\n        self.num_positions = self.num_patches\n        self.position_embedding = nn.Embedding(self.num_positions, self.embed_dim)\n        self.register_buffer(\n            \"position_ids\",\n            torch.arange(self.num_positions).expand((1, -1)),\n            persistent=False,\n        )\n\n    def forward(self, pixel_values: torch.FloatTensor) -> torch.Tensor:\n        _, _, height, width = pixel_values.shape # [Batch_Size, Channels, Height, Width]\n        # Convolve the `patch_size` kernel over the image, with no overlapping patches since the stride is equal to the kernel size\n        # The output of the convolution will have shape [Batch_Size, Embed_Dim, Num_Patches_H, Num_Patches_W]\n        # where Num_Patches_H = height // patch_size and Num_Patches_W = width // patch_size\n        patch_embeds = self.patch_embedding(pixel_values)  \n        # [Batch_Size, Embed_Dim, Num_Patches_H, Num_Patches_W] -> [Batch_Size, Embed_Dim, Num_Patches]\n        # where Num_Patches = Num_Patches_H * Num_Patches_W\n        embeddings = patch_embeds.flatten(2)\n        # [Batch_Size, Embed_Dim, Num_Patches] -> [Batch_Size, Num_Patches, Embed_Dim]\n        embeddings = embeddings.transpose(1, 2)\n        # Add position embeddings to each patch. Each positional encoding is a vector of size [Embed_Dim]\n        embeddings = embeddings + self.position_embedding(self.position_ids)\n        # [Batch_Size, Num_Patches, Embed_Dim]\n        return embeddings\n\n\nclass SiglipAttention(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.embed_dim = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.embed_dim // self.num_heads\n        self.scale = self.head_dim**-0.5 # Equivalent to 1 / sqrt(self.head_dim)\n        self.dropout = config.attention_dropout\n\n        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)\n        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)\n        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n\n        # hidden_states: [Batch_Size, Num_Patches, Embed_Dim]\n        batch_size, seq_len, _ = hidden_states.size()\n        # query_states: [Batch_Size, Num_Patches, Embed_Dim]\n        query_states = self.q_proj(hidden_states)\n        # key_states: [Batch_Size, Num_Patches, Embed_Dim]\n        key_states = self.k_proj(hidden_states)\n        # value_states: [Batch_Size, Num_Patches, Embed_Dim]\n        value_states = self.v_proj(hidden_states)\n        # query_states: [Batch_Size, Num_Heads, Num_Patches, Head_Dim]\n        query_states = query_states.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n\n        key_states = key_states.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n\n        value_states = value_states.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n        # Calculate the attention using the formula Q * K^T / sqrt(d_k). attn_weights: [Batch_Size, Num_Heads, Num_Patches, Num_Patches]\n        attn_weights = (torch.matmul(query_states, key_states.transpose(2, 3)) * self.scale)\n\n        if attn_weights.size() != (batch_size, self.num_heads, seq_len, seq_len):\n            raise ValueError(\n                f\"Attention weights should be of size {(batch_size, self.num_heads, seq_len, seq_len)}, but is\"\n                f\" {attn_weights.size()}\"\n            )\n\n        # Apply the softmax row-wise. attn_weights: [Batch_Size, Num_Heads, Num_Patches, Num_Patches]\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n        # Apply dropout only during training\n        attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n        # Multiply the attention weights by the value states. attn_output: [Batch_Size, Num_Heads, Num_Patches, Head_Dim]\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        if attn_output.size() != (batch_size, self.num_heads, seq_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(batch_size, self.num_heads, seq_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n        # [Batch_Size, Num_Heads, Num_Patches, Head_Dim] -> [Batch_Size, Num_Patches, Num_Heads, Head_Dim]\n        attn_output = attn_output.transpose(1, 2).contiguous()\n        # [Batch_Size, Num_Patches, Num_Heads, Head_Dim] -> [Batch_Size, Num_Patches, Embed_Dim]\n        attn_output = attn_output.reshape(batch_size, seq_len, self.embed_dim)\n        # [Batch_Size, Num_Patches, Embed_Dim]\n        attn_output = self.out_proj(attn_output)\n\n        return attn_output, attn_weights\n\n\nclass SiglipMLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)\n        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n\n    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n        # [Batch_Size, Num_Patches, Embed_Dim] -> [Batch_Size, Num_Patches, Intermediate_Size]\n        hidden_states = self.fc1(hidden_states)\n        # hidden_states: [Batch_Size, Num_Patches, Intermediate_Size]\n        hidden_states = nn.functional.gelu(hidden_states, approximate=\"tanh\")\n        # [Batch_Size, Num_Patches, Intermediate_Size] -> [Batch_Size, Num_Patches, Embed_Dim]\n        hidden_states = self.fc2(hidden_states)\n\n        return hidden_states\n\n\nclass SiglipEncoderLayer(nn.Module):\n    def __init__(self, config: SiglipVisionConfig):\n        super().__init__()\n        self.embed_dim = config.hidden_size\n        self.self_attn = SiglipAttention(config)\n        self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n        self.mlp = SiglipMLP(config)\n        self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n\n    # Ignore copy\n    def forward(\n        self,\n        hidden_states: torch.Tensor\n    ) -> torch.Tensor:\n        # residual: [Batch_Size, Num_Patches, Embed_Dim]\n        residual = hidden_states\n        # [Batch_Size, Num_Patches, Embed_Dim] -> [Batch_Size, Num_Patches, Embed_Dim]\n        hidden_states = self.layer_norm1(hidden_states)\n        # [Batch_Size, Num_Patches, Embed_Dim] -> [Batch_Size, Num_Patches, Embed_Dim]\n        hidden_states, _ = self.self_attn(hidden_states=hidden_states)\n        # [Batch_Size, Num_Patches, Embed_Dim]\n        hidden_states = residual + hidden_states\n        # residual: [Batch_Size, Num_Patches, Embed_Dim] \n        residual = hidden_states\n        # [Batch_Size, Num_Patches, Embed_Dim] -> [Batch_Size, Num_Patches, Embed_Dim]\n        hidden_states = self.layer_norm2(hidden_states)\n        # [Batch_Size, Num_Patches, Embed_Dim] -> [Batch_Size, Num_Patches, Embed_Dim]\n        hidden_states = self.mlp(hidden_states)\n        # [Batch_Size, Num_Patches, Embed_Dim]\n        hidden_states = residual + hidden_states\n        \n        return hidden_states\n\n\nclass SiglipEncoder(nn.Module):\n    def __init__(self, config: SiglipVisionConfig):\n        super().__init__()\n        self.config = config\n        self.layers = nn.ModuleList(\n            [SiglipEncoderLayer(config) for _ in range(config.num_hidden_layers)]\n        )\n\n    # Ignore copy\n    def forward(\n        self,\n        inputs_embeds: torch.Tensor\n    ) -> torch.Tensor:\n        # inputs_embeds: [Batch_Size, Num_Patches, Embed_Dim]\n        hidden_states = inputs_embeds\n\n        for encoder_layer in self.layers:\n            # [Batch_Size, Num_Patches, Embed_Dim] -> [Batch_Size, Num_Patches, Embed_Dim]\n            hidden_states = encoder_layer(hidden_states)\n\n        return hidden_states\n\n\nclass SiglipVisionTransformer(nn.Module):\n    def __init__(self, config: SiglipVisionConfig):\n        super().__init__()\n        self.config = config\n        embed_dim = config.hidden_size\n\n        self.embeddings = SiglipVisionEmbeddings(config)\n        self.encoder = SiglipEncoder(config)\n        self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n\n    def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n        # pixel_values: [Batch_Size, Channels, Height, Width] -> [Batch_Size, Num_Patches, Embed_Dim]\n        hidden_states = self.embeddings(pixel_values)\n\n        last_hidden_state = self.encoder(inputs_embeds=hidden_states)\n\n        last_hidden_state = self.post_layernorm(last_hidden_state)\n\n        return last_hidden_state\n\n\nclass SiglipVisionModel(nn.Module):\n\n    def __init__(self, config: SiglipVisionConfig):\n        super().__init__()\n        self.config = config\n        self.vision_model = SiglipVisionTransformer(config)\n\n    def forward(self, pixel_values) -> Tuple:\n        # [Batch_Size, Channels, Height, Width] -> [Batch_Size, Num_Patches, Embed_Dim]\n        return self.vision_model(pixel_values=pixel_values) ","metadata":{"_uuid":"ce67c642-71a0-4321-9552-baa2079e993a","_cell_guid":"5b259ec2-a3b2-4da6-a5ea-8237f25dfc67","trusted":true,"id":"HXJQwCbs4n4S","executionInfo":{"status":"ok","timestamp":1730990305861,"user_tz":-330,"elapsed":552,"user":{"displayName":"","userId":""}},"outputId":"67384385-199e-4c23-f139-b80872ee0697","execution":{"iopub.status.busy":"2024-11-16T12:03:18.536754Z","iopub.execute_input":"2024-11-16T12:03:18.537375Z","iopub.status.idle":"2024-11-16T12:03:18.550537Z","shell.execute_reply.started":"2024-11-16T12:03:18.537334Z","shell.execute_reply":"2024-11-16T12:03:18.549554Z"}},"outputs":[{"name":"stdout","text":"Writing modeling_siglip.py\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Processing_Paligemma","metadata":{}},{"cell_type":"code","source":"%%writefile processing_paligemma.py\n\nfrom typing import Dict, List, Optional, Union, Tuple, Iterable\nimport numpy as np\nfrom PIL import Image\nimport torch\n\nIMAGENET_STANDARD_MEAN = [0.5, 0.5, 0.5]\nIMAGENET_STANDARD_STD = [0.5, 0.5, 0.5]\n\n\ndef add_image_tokens_to_prompt(prefix_prompt, bos_token, image_seq_len, image_token):\n    # Quoting from the blog (https://huggingface.co/blog/paligemma#detailed-inference-process):\n    #   The input text is tokenized normally.\n    #   A <bos> token is added at the beginning, and an additional newline token (\\n) is appended.\n    #   This newline token is an essential part of the input prompt the model was trained with, so adding it explicitly ensures it's always there.\n    #   The tokenized text is also prefixed with a fixed number of <image> tokens.\n    # NOTE: from the paper it looks like the `\\n` should be tokenized separately, but in the HF implementation this is not done.\n    #       ref to HF implementation: https://github.com/huggingface/transformers/blob/7f79a97399bb52aad8460e1da2f36577d5dccfed/src/transformers/models/paligemma/processing_paligemma.py#L55-L73\n    return f\"{image_token * image_seq_len}{bos_token}{prefix_prompt}\\n\"\n\n\ndef rescale(\n    image: np.ndarray, scale: float, dtype: np.dtype = np.float32\n) -> np.ndarray:\n    rescaled_image = image * scale\n    rescaled_image = rescaled_image.astype(dtype)\n    return rescaled_image\n\n\ndef resize(\n    image: Image,\n    size: Tuple[int, int],\n    resample: Image.Resampling = None,\n    reducing_gap: Optional[int] = None,\n) -> np.ndarray:\n    height, width = size\n    resized_image = image.resize(\n        (width, height), resample=resample, reducing_gap=reducing_gap\n    )\n    return resized_image\n\n\ndef normalize(\n    image: np.ndarray,\n    mean: Union[float, Iterable[float]],\n    std: Union[float, Iterable[float]],\n) -> np.ndarray:\n    mean = np.array(mean, dtype=image.dtype)\n    std = np.array(std, dtype=image.dtype)\n    image = (image - mean) / std\n    return image\n\n\ndef process_images(\n    images: List[Image.Image],\n    size: Dict[str, int] = None,\n    resample: Image.Resampling = None,\n    rescale_factor: float = None,\n    image_mean: Optional[Union[float, List[float]]] = None,\n    image_std: Optional[Union[float, List[float]]] = None,\n) -> List[np.ndarray]:\n    height, width = size[0], size[1]\n    images = [\n        resize(image=image, size=(height, width), resample=resample) for image in images\n    ]\n    # Convert each image to a numpy array\n    images = [np.array(image) for image in images]\n    # Rescale the pixel values to be in the range [0, 1]\n    images = [rescale(image, scale=rescale_factor) for image in images]\n    # Normalize the images to have mean 0 and standard deviation 1\n    images = [normalize(image, mean=image_mean, std=image_std) for image in images]\n    # Move the channel dimension to the first dimension. The model expects images in the format [Channel, Height, Width]\n    images = [image.transpose(2, 0, 1) for image in images]\n    return images\n\n\nclass PaliGemmaProcessor:\n\n    IMAGE_TOKEN = \"<image>\"\n\n    def __init__(self, tokenizer, num_image_tokens: int, image_size: int):\n        super().__init__()\n\n        self.image_seq_length = num_image_tokens\n        self.image_size = image_size\n\n        # Tokenizer described here: https://github.com/google-research/big_vision/blob/main/big_vision/configs/proj/paligemma/README.md#tokenizer\n        tokens_to_add = {\"additional_special_tokens\": [self.IMAGE_TOKEN]}\n        tokenizer.add_special_tokens(tokens_to_add)\n        EXTRA_TOKENS = [\n            f\"<loc{i:04d}>\" for i in range(1024)\n        ]  # These tokens are used for object detection (bounding boxes)\n        EXTRA_TOKENS += [\n            f\"<seg{i:03d}>\" for i in range(128)\n        ]  # These tokens are used for object segmentation\n        tokenizer.add_tokens(EXTRA_TOKENS)\n        self.image_token_id = tokenizer.convert_tokens_to_ids(self.IMAGE_TOKEN)\n        # We will add the BOS and EOS tokens ourselves\n        tokenizer.add_bos_token = False\n        tokenizer.add_eos_token = False\n\n        self.tokenizer = tokenizer\n\n    def __call__(\n        self,\n        text: List[str],\n        images: List[Image.Image],\n        padding: str = \"longest\",\n        truncation: bool = True,\n    ) -> dict:\n        assert len(images) == 1 and len(text) == 1, f\"Received {len(images)} images for {len(text)} prompts.\"\n\n        pixel_values = process_images(\n            images,\n            size=(self.image_size, self.image_size),\n            resample=Image.Resampling.BICUBIC,\n            rescale_factor=1 / 255.0,\n            image_mean=IMAGENET_STANDARD_MEAN,\n            image_std=IMAGENET_STANDARD_STD,\n        )\n        # Convert the list of numpy arrays to a single numpy array with shape [Batch_Size, Channel, Height, Width]\n        pixel_values = np.stack(pixel_values, axis=0)\n        # Convert the numpy array to a PyTorch tensor\n        pixel_values = torch.tensor(pixel_values)\n\n        # Prepend a `self.image_seq_length` number of image tokens to the prompt\n        input_strings = [\n            add_image_tokens_to_prompt(\n                prefix_prompt=prompt,\n                bos_token=self.tokenizer.bos_token,\n                image_seq_len=self.image_seq_length,\n                image_token=self.IMAGE_TOKEN,\n            )\n            for prompt in text\n        ]\n\n        # Returns the input_ids and attention_mask as PyTorch tensors\n        inputs = self.tokenizer(\n            input_strings,\n            return_tensors=\"pt\",\n            padding=padding,\n            truncation=truncation,\n        )\n\n        return_data = {\"pixel_values\": pixel_values, **inputs}\n\n        return return_data","metadata":{"_uuid":"7399a3f6-5b4e-4be4-8fdc-19fb8a450647","_cell_guid":"7259275c-9e43-4f50-a0e3-c96063eff5e4","trusted":true,"id":"4InrpSVq4n4d","executionInfo":{"status":"ok","timestamp":1730990306523,"user_tz":-330,"elapsed":8,"user":{"displayName":"","userId":""}},"outputId":"1be80213-10dd-436e-c4c3-bd487268245e","execution":{"iopub.status.busy":"2024-11-16T12:03:19.795379Z","iopub.execute_input":"2024-11-16T12:03:19.795998Z","iopub.status.idle":"2024-11-16T12:03:19.805174Z","shell.execute_reply.started":"2024-11-16T12:03:19.795960Z","shell.execute_reply":"2024-11-16T12:03:19.804147Z"}},"outputs":[{"name":"stdout","text":"Writing processing_paligemma.py\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# Modeling_Gemma","metadata":{}},{"cell_type":"code","source":"%%writefile modeling_gemma.py\n\nimport torch\nfrom torch import nn\nfrom typing import Optional, Tuple, List\nfrom torch.nn import CrossEntropyLoss\nimport math\nfrom modeling_siglip import SiglipVisionConfig, SiglipVisionModel\n\nclass KVCache():\n\n    def __init__(self) -> None:\n        self.key_cache: List[torch.Tensor] = []\n        self.value_cache: List[torch.Tensor] = []\n    \n    def num_items(self) -> int:\n        if len(self.key_cache) == 0:\n            return 0\n        else:\n            # The shape of the key_cache is [Batch_Size, Num_Heads_KV, Seq_Len, Head_Dim]\n            return self.key_cache[0].shape[-2]\n\n    def update(\n        self,\n        key_states: torch.Tensor,\n        value_states: torch.Tensor,\n        layer_idx: int,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        if len(self.key_cache) <= layer_idx:\n            # If we never added anything to the KV-Cache of this layer, let's create it.\n            self.key_cache.append(key_states)\n            self.value_cache.append(value_states)\n        else:\n            # ... otherwise we concatenate the new keys with the existing ones.\n            # each tensor has shape: [Batch_Size, Num_Heads_KV, Seq_Len, Head_Dim]\n            self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2)\n            self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2)\n\n        # ... and then we return all the existing keys + the new ones.\n        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n\nclass GemmaConfig():\n\n    def __init__(\n        self,\n        vocab_size,\n        hidden_size,\n        intermediate_size,\n        num_hidden_layers,\n        num_attention_heads,\n        num_key_value_heads,\n        head_dim=256,\n        max_position_embeddings=8192,\n        rms_norm_eps=1e-6,\n        rope_theta=10000.0,\n        attention_bias=False,\n        attention_dropout=0.0,\n        pad_token_id=None,\n        **kwargs,\n    ):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.max_position_embeddings = max_position_embeddings\n        self.hidden_size = hidden_size\n        self.intermediate_size = intermediate_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.head_dim = head_dim\n        self.num_key_value_heads = num_key_value_heads\n        self.rms_norm_eps = rms_norm_eps\n        self.rope_theta = rope_theta\n        self.attention_bias = attention_bias\n        self.attention_dropout = attention_dropout\n        self.pad_token_id = pad_token_id\n\nclass PaliGemmaConfig():\n\n    def __init__(\n        self,\n        vision_config=None,\n        text_config=None,\n        ignore_index=-100,\n        image_token_index=256000,\n        vocab_size=257152,\n        projection_dim=2048,\n        hidden_size=2048,\n        pad_token_id=None,\n        **kwargs,\n    ):\n        super().__init__()\n        self.ignore_index = ignore_index\n        self.image_token_index = image_token_index\n        self.vocab_size = vocab_size\n        self.projection_dim = projection_dim\n        self.hidden_size = hidden_size\n        self.vision_config = vision_config\n        self.is_encoder_decoder = False\n        self.pad_token_id = pad_token_id\n\n        self.vision_config = SiglipVisionConfig(**vision_config)\n        self.text_config = text_config\n\n        self.text_config = GemmaConfig(**text_config, pad_token_id=pad_token_id)\n        self.vocab_size = self.text_config.vocab_size\n\n        self.text_config.num_image_tokens = (self.vision_config.image_size // self.vision_config.patch_size) ** 2\n        self.vision_config.projection_dim = projection_dim\n\n\nclass GemmaRMSNorm(nn.Module):\n    def __init__(self, dim: int, eps: float = 1e-6):\n        super().__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.zeros(dim))\n\n    def _norm(self, x):\n        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n\n    def forward(self, x):\n        output = self._norm(x.float())\n        # Llama does x.to(float16) * w whilst Gemma is (x * w).to(float16)\n        # See https://github.com/huggingface/transformers/pull/29402\n        output = output * (1.0 + self.weight.float())\n        return output.type_as(x)\n\nclass GemmaRotaryEmbedding(nn.Module):\n    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n        super().__init__()\n\n        self.dim = dim # it is set to the head_dim\n        self.max_position_embeddings = max_position_embeddings\n        self.base = base\n\n        # Calculate the theta according to the formula theta_i = base^(2i/dim) where i = 0, 1, 2, ..., dim // 2\n        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float() / self.dim))\n        self.register_buffer(\"inv_freq\", tensor=inv_freq, persistent=False)\n\n    @torch.no_grad()\n    def forward(self, x, position_ids, seq_len=None):\n        # x: [bs, num_attention_heads, seq_len, head_size]\n        self.inv_freq.to(x.device)\n        # Copy the inv_freq tensor for batch in the sequence\n        # inv_freq_expanded: [Batch_Size, Head_Dim // 2, 1]\n        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n        # position_ids_expanded: [Batch_Size, 1, Seq_Len]\n        position_ids_expanded = position_ids[:, None, :].float()\n        device_type = x.device.type\n        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n        with torch.autocast(device_type=device_type, enabled=False):\n            # Multiply each theta by the position (which is the argument of the sin and cos functions)\n            # freqs: [Batch_Size, Head_Dim // 2, 1] @ [Batch_Size, 1, Seq_Len] --> [Batch_Size, Seq_Len, Head_Dim // 2]\n            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n            # emb: [Batch_Size, Seq_Len, Head_Dim]\n            emb = torch.cat((freqs, freqs), dim=-1)\n            # cos, sin: [Batch_Size, Seq_Len, Head_Dim]\n            cos = emb.cos()\n            sin = emb.sin()\n        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n\n\ndef rotate_half(x):\n    # Build the [-x2, x1, -x4, x3, ...] tensor for the sin part of the positional encoding.\n    x1 = x[..., : x.shape[-1] // 2] # Takes the first half of the last dimension\n    x2 = x[..., x.shape[-1] // 2 :] # Takes the second half of the last dimension\n    return torch.cat((-x2, x1), dim=-1)\n\n\ndef apply_rotary_pos_emb(q, k, cos, sin, unsqueeze_dim=1):\n    cos = cos.unsqueeze(unsqueeze_dim) # Add the head dimension\n    sin = sin.unsqueeze(unsqueeze_dim) # Add the head dimension\n    # Apply the formula (34) of the Rotary Positional Encoding paper.\n    q_embed = (q * cos) + (rotate_half(q) * sin)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return q_embed, k_embed\n\n\nclass GemmaMLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.hidden_size = config.hidden_size\n        self.intermediate_size = config.intermediate_size\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n\n    def forward(self, x):\n        # Equivalent to:\n        # y = self.gate_proj(x) # [Batch_Size, Seq_Len, Hidden_Size] -> [Batch_Size, Seq_Len, Intermediate_Size]\n        # y = torch.gelu(y, approximate=\"tanh\") # [Batch_Size, Seq_Len, Intermediate_Size]\n        # j = self.up_proj(x) # [Batch_Size, Seq_Len, Hidden_Size] -> [Batch_Size, Seq_Len, Intermediate_Size]\n        # z = y * j # [Batch_Size, Seq_Len, Intermediate_Size]\n        # z = self.down_proj(z) # [Batch_Size, Seq_Len, Intermediate_Size] -> [Batch_Size, Seq_Len, Hidden_Size]\n        return self.down_proj(nn.functional.gelu(self.gate_proj(x), approximate=\"tanh\") * self.up_proj(x))\n\ndef repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n    if n_rep == 1:\n        return hidden_states\n    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n\nclass GemmaAttention(nn.Module):\n\n    def __init__(self, config: GemmaConfig, layer_idx: Optional[int] = None):\n        super().__init__()\n        self.config = config\n        self.layer_idx = layer_idx\n\n        self.attention_dropout = config.attention_dropout\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = config.head_dim\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n        self.rope_theta = config.rope_theta\n        self.is_causal = True\n\n        assert self.hidden_size % self.num_heads == 0            \n\n        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n        self.rotary_emb = GemmaRotaryEmbedding(\n            self.head_dim,\n            max_position_embeddings=self.max_position_embeddings,\n            base=self.rope_theta,\n        )\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        kv_cache: Optional[KVCache] = None,\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        bsz, q_len, _ = hidden_states.size() # [Batch_Size, Seq_Len, Hidden_Size]\n        # [Batch_Size, Seq_Len, Num_Heads_Q * Head_Dim]\n        query_states = self.q_proj(hidden_states)\n        # [Batch_Size, Seq_Len, Num_Heads_KV * Head_Dim]\n        key_states = self.k_proj(hidden_states)\n        # [Batch_Size, Seq_Len, Num_Heads_KV * Head_Dim]\n        value_states = self.v_proj(hidden_states)\n        # [Batch_Size, Num_Heads_Q, Seq_Len, Head_Dim]\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        # [Batch_Size, Num_Heads_KV, Seq_Len, Head_Dim]\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        # [Batch_Size, Num_Heads_KV, Seq_Len, Head_Dim]\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        # [Batch_Size, Seq_Len, Head_Dim], [Batch_Size, Seq_Len, Head_Dim]\n        cos, sin = self.rotary_emb(value_states, position_ids, seq_len=None)\n        # [Batch_Size, Num_Heads_Q, Seq_Len, Head_Dim], [Batch_Size, Num_Heads_KV, Seq_Len, Head_Dim]\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n        if kv_cache is not None:\n            key_states, value_states = kv_cache.update(key_states, value_states, self.layer_idx)\n\n        # Repeat the key and values to match the number of heads of the query\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n        # Perform the calculation as usual, Q * K^T / sqrt(head_dim). Shape: [Batch_Size, Num_Heads_Q, Seq_Len_Q, Seq_Len_KV]\n        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n        assert attention_mask is not None\n        attn_weights = attn_weights + attention_mask\n\n        # Apply the softmax\n        # [Batch_Size, Num_Heads_Q, Seq_Len_Q, Seq_Len_KV]\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n        # Apply the dropout\n        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n        # Multiply by the values. [Batch_Size, Num_Heads_Q, Seq_Len_Q, Seq_Len_KV] x [Batch_Size, Num_Heads_KV, Seq_Len_KV, Head_Dim] -> [Batch_Size, Num_Heads_Q, Seq_Len_Q, Head_Dim]\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n        # Make sure the sequence length is the second dimension. # [Batch_Size, Num_Heads_Q, Seq_Len_Q, Head_Dim] -> [Batch_Size, Seq_Len_Q, Num_Heads_Q, Head_Dim]\n        attn_output = attn_output.transpose(1, 2).contiguous()\n        # Concatenate all the heads together. [Batch_Size, Seq_Len_Q, Num_Heads_Q, Head_Dim] -> [Batch_Size, Seq_Len_Q, Num_Heads_Q * Head_Dim]\n        attn_output = attn_output.view(bsz, q_len, -1)\n        # Multiply by W_o. [Batch_Size, Seq_Len_Q, Hidden_Size]\n        attn_output = self.o_proj(attn_output)\n\n        return attn_output, attn_weights\n\nclass GemmaDecoderLayer(nn.Module):\n\n    def __init__(self, config: GemmaConfig, layer_idx: int):\n        super().__init__()\n        self.hidden_size = config.hidden_size\n\n        self.self_attn = GemmaAttention(config=config, layer_idx=layer_idx)\n\n        self.mlp = GemmaMLP(config)\n        self.input_layernorm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.post_attention_layernorm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        kv_cache: Optional[KVCache] = None,\n    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n        residual = hidden_states\n        # [Batch_Size, Seq_Len, Hidden_Size]\n        hidden_states = self.input_layernorm(hidden_states)\n\n        # [Batch_Size, Seq_Len, Hidden_Size]\n        hidden_states, _, = self.self_attn(\n            hidden_states=hidden_states,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            kv_cache=kv_cache,\n        )\n        # [Batch_Size, Seq_Len, Hidden_Size]\n        hidden_states = residual + hidden_states\n\n        # [Batch_Size, Seq_Len, Hidden_Size]\n        residual = hidden_states\n        # [Batch_Size, Seq_Len, Hidden_Size]\n        hidden_states = self.post_attention_layernorm(hidden_states)\n        # [Batch_Size, Seq_Len, Hidden_Size]\n        hidden_states = self.mlp(hidden_states)\n        # [Batch_Size, Seq_Len, Hidden_Size]\n        hidden_states = residual + hidden_states\n\n        return hidden_states\n\nclass GemmaModel(nn.Module):\n\n    def __init__(self, config: GemmaConfig):\n        super().__init__()\n        self.config = config\n        self.padding_idx = config.pad_token_id\n        self.vocab_size = config.vocab_size\n\n        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n        self.layers = nn.ModuleList(\n            [GemmaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n        )\n        self.norm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n    def get_input_embeddings(self):\n        return self.embed_tokens\n\n    # Ignore copy\n    def forward(\n        self,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        kv_cache: Optional[KVCache] = None,\n    ) -> torch.FloatTensor:\n        # [Batch_Size, Seq_Len, Hidden_Size]\n        hidden_states = inputs_embeds\n        # [Batch_Size, Seq_Len, Hidden_Size]\n        normalizer = torch.tensor(self.config.hidden_size**0.5, dtype=hidden_states.dtype)\n        hidden_states = hidden_states * normalizer\n\n        for decoder_layer in self.layers:\n            # [Batch_Size, Seq_Len, Hidden_Size]\n            hidden_states = decoder_layer(\n                hidden_states,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                kv_cache=kv_cache,\n            )\n\n        # [Batch_Size, Seq_Len, Hidden_Size]\n        hidden_states = self.norm(hidden_states)\n\n        # [Batch_Size, Seq_Len, Hidden_Size]\n        return hidden_states\n\nclass GemmaForCausalLM(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.model = GemmaModel(config)\n        self.vocab_size = config.vocab_size\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n    def get_input_embeddings(self):\n        return self.model.embed_tokens\n    \n    def tie_weights(self):\n        self.lm_head.weight = self.model.embed_tokens.weight\n\n    def forward(\n        self,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        kv_cache: Optional[KVCache] = None,\n    ) -> Tuple:\n\n        # input_embeds: [Batch_Size, Seq_Len, Hidden_Size]\n        # outputs: [Batch_Size, Seq_Len, Hidden_Size]\n        outputs = self.model(\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            inputs_embeds=inputs_embeds,\n            kv_cache=kv_cache,\n        )\n\n        hidden_states = outputs\n        logits = self.lm_head(hidden_states)\n        logits = logits.float()\n\n        return_data = {\n            \"logits\": logits,\n        }\n\n        if kv_cache is not None:\n            # Return the updated cache\n            return_data[\"kv_cache\"] = kv_cache\n\n        return return_data\n\nclass PaliGemmaMultiModalProjector(nn.Module):\n    def __init__(self, config: PaliGemmaConfig):\n        super().__init__()\n        self.linear = nn.Linear(config.vision_config.hidden_size, config.vision_config.projection_dim, bias=True)\n\n    def forward(self, image_features):\n        # [Batch_Size, Num_Patches, Embed_Dim] -> [Batch_Size, Num_Patches, Projection_Dim]\n        hidden_states = self.linear(image_features)\n        return hidden_states\n\nclass PaliGemmaForConditionalGeneration(nn.Module):\n    def __init__(self, config: PaliGemmaConfig):\n        super().__init__()\n        self.config = config\n        self.vision_tower = SiglipVisionModel(config.vision_config)\n        self.multi_modal_projector = PaliGemmaMultiModalProjector(config)\n        self.vocab_size = config.vocab_size\n\n        language_model = GemmaForCausalLM(config.text_config)\n        self.language_model = language_model\n\n        self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n\n    def tie_weights(self):\n        return self.language_model.tie_weights()\n\n    def _merge_input_ids_with_image_features(\n        self, image_features: torch.Tensor, inputs_embeds: torch.Tensor, input_ids: torch.Tensor, attention_mask: torch.Tensor, kv_cache: Optional[KVCache] = None\n    ):\n        _, _, embed_dim = image_features.shape\n        batch_size, sequence_length = input_ids.shape\n        dtype, device = inputs_embeds.dtype, inputs_embeds.device\n        # Shape: [Batch_Size, Seq_Len, Hidden_Size]\n        scaled_image_features = image_features / (self.config.hidden_size**0.5)\n    \n        # Combine the embeddings of the image tokens, the text tokens and mask out all the padding tokens.\n        final_embedding = torch.zeros(batch_size, sequence_length, embed_dim, dtype=inputs_embeds.dtype, device=inputs_embeds.device)\n        # Shape: [Batch_Size, Seq_Len]. True for text tokens\n        text_mask = (input_ids != self.config.image_token_index) & (input_ids != self.pad_token_id)\n        # Shape: [Batch_Size, Seq_Len]. True for image tokens\n        image_mask = input_ids == self.config.image_token_index\n        # Shape: [Batch_Size, Seq_Len]. True for padding tokens\n        pad_mask = input_ids == self.pad_token_id\n\n        # We need to expand the masks to the embedding dimension otherwise we can't use them in torch.where\n        text_mask_expanded = text_mask.unsqueeze(-1).expand(-1, -1, embed_dim)\n        pad_mask_expanded = pad_mask.unsqueeze(-1).expand(-1, -1, embed_dim)\n        image_mask_expanded = image_mask.unsqueeze(-1).expand(-1, -1, embed_dim)\n\n        # Add the text embeddings\n        final_embedding = torch.where(text_mask_expanded, inputs_embeds, final_embedding)\n        # Insert image embeddings. We can't use torch.where because the sequence length of scaled_image_features is not equal to the sequence length of the final embedding\n        final_embedding = final_embedding.masked_scatter(image_mask_expanded, scaled_image_features)\n        # Zero out padding tokens\n        final_embedding = torch.where(pad_mask_expanded, torch.zeros_like(final_embedding), final_embedding)\n\n        #### CREATE THE ATTENTION MASK ####\n\n        dtype, device = inputs_embeds.dtype, inputs_embeds.device\n        min_dtype = torch.finfo(dtype).min\n        q_len = inputs_embeds.shape[1]\n    \n        if kv_cache is None or kv_cache.num_items() == 0:\n            # Do not mask any token, because we're in the prefill phase\n            # This only works when we have no padding\n            causal_mask = torch.full(\n                (batch_size, q_len, q_len), fill_value=0, dtype=dtype, device=device\n            )\n        else:\n            # Since we are generating tokens, the query must be one single token\n            assert q_len == 1\n            kv_len = kv_cache.num_items() + q_len\n            # Also in this case we don't need to mask anything, since each query should be able to attend all previous tokens. \n            # This only works when we have no padding\n            causal_mask = torch.full(\n                (batch_size, q_len, kv_len), fill_value=0, dtype=dtype, device=device\n            )\n\n        # Add the head dimension\n        # [Batch_Size, Q_Len, KV_Len] -> [Batch_Size, Num_Heads_Q, Q_Len, KV_Len]\n        causal_mask = causal_mask.unsqueeze(1)\n\n        if kv_cache is not None and kv_cache.num_items() > 0:\n            # The position of the query is just the last position\n            position_ids = attention_mask.cumsum(-1)[:, -1]\n            if position_ids.dim() == 1:\n                position_ids = position_ids.unsqueeze(0)\n        else:\n            # Create a position_ids based on the size of the attention_mask\n            # For masked tokens, use the number 1 as position.\n            position_ids = (attention_mask.cumsum(-1)).masked_fill_((attention_mask == 0), 1).to(device)\n\n        return final_embedding, causal_mask, position_ids\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        pixel_values: torch.FloatTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        kv_cache: Optional[KVCache] = None,\n    ) -> Tuple:\n\n        # Make sure the input is right-padded\n        assert torch.all(attention_mask == 1), \"The input cannot be padded\"\n\n        # 1. Extra the input embeddings\n        # shape: (Batch_Size, Seq_Len, Hidden_Size)\n        inputs_embeds = self.language_model.get_input_embeddings()(input_ids)\n\n        # 2. Merge text and images\n        # [Batch_Size, Channels, Height, Width] -> [Batch_Size, Num_Patches, Embed_Dim]\n        selected_image_feature = self.vision_tower(pixel_values.to(inputs_embeds.dtype))\n        # [Batch_Size, Num_Patches, Embed_Dim] -> [Batch_Size, Num_Patches, Hidden_Size]\n        image_features = self.multi_modal_projector(selected_image_feature)\n\n        # Merge the embeddings of the text tokens and the image tokens\n        inputs_embeds, attention_mask, position_ids = self._merge_input_ids_with_image_features(image_features, inputs_embeds, input_ids, attention_mask, kv_cache)\n        \n        outputs = self.language_model(\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            inputs_embeds=inputs_embeds,\n            kv_cache=kv_cache,\n        )\n\n        return outputs","metadata":{"_uuid":"b4364f0f-a8d7-4666-b73a-aa50c5ead9cf","_cell_guid":"955ea4db-c09f-4a3a-9930-78d747ff5d4b","trusted":true,"_kg_hide-input":true,"id":"5KM2Xmb54n4f","executionInfo":{"status":"ok","timestamp":1730990307253,"user_tz":-330,"elapsed":6,"user":{"displayName":"","userId":""}},"outputId":"2e4fa4f2-f234-4533-d428-8cc70ffcbffe","execution":{"iopub.status.busy":"2024-11-16T12:03:20.978349Z","iopub.execute_input":"2024-11-16T12:03:20.978682Z","iopub.status.idle":"2024-11-16T12:03:20.999845Z","shell.execute_reply.started":"2024-11-16T12:03:20.978650Z","shell.execute_reply":"2024-11-16T12:03:20.998924Z"}},"outputs":[{"name":"stdout","text":"Writing modeling_gemma.py\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"%%writefile utils.py\n\nfrom modeling_gemma import PaliGemmaForConditionalGeneration, PaliGemmaConfig\nfrom transformers import AutoTokenizer\nimport json\nimport glob\nfrom safetensors import safe_open\nfrom typing import Tuple\nimport os\n\ndef load_hf_model(model_path: str, device: str) -> Tuple[PaliGemmaForConditionalGeneration, AutoTokenizer]:\n    # Load the tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_path, truncation=True, padding_side=\"right\")\n    assert tokenizer.padding_side == \"right\"\n\n    # Find all the *.safetensors files\n    safetensors_files = glob.glob(os.path.join(model_path, \"*.safetensors\"))\n\n    # ... and load them one by one in the tensors dictionary\n    tensors = {}\n    for safetensors_file in safetensors_files:\n        with safe_open(safetensors_file, framework=\"pt\", device=\"cpu\") as f:\n            for key in f.keys():\n                tensors[key] = f.get_tensor(key)\n\n    # Load the model's config\n    with open(os.path.join(model_path, \"config.json\"), \"r\") as f:\n        model_config_file = json.load(f)\n        config = PaliGemmaConfig(**model_config_file)\n\n    # Create the model using the configuration\n    model = PaliGemmaForConditionalGeneration(config).to(device)\n\n    # Load the state dict of the model\n    model.load_state_dict(tensors, strict=False)\n\n    # Tie weights\n    model.tie_weights()\n\n    return (model, tokenizer)","metadata":{"_uuid":"f6e4e3ed-cd5d-4a0d-a2bb-9f2cbc47a717","_cell_guid":"1c71a741-3f8e-44c8-953f-fad6a64391e6","trusted":true,"id":"qRt_GGKP4n4h","executionInfo":{"status":"ok","timestamp":1730990307762,"user_tz":-330,"elapsed":6,"user":{"displayName":"","userId":""}},"outputId":"adbeeee7-db7f-4fe1-f3d7-34a09ddbffb2","execution":{"iopub.status.busy":"2024-11-16T12:03:21.630146Z","iopub.execute_input":"2024-11-16T12:03:21.630882Z","iopub.status.idle":"2024-11-16T12:03:21.636876Z","shell.execute_reply.started":"2024-11-16T12:03:21.630842Z","shell.execute_reply":"2024-11-16T12:03:21.635967Z"}},"outputs":[{"name":"stdout","text":"Writing utils.py\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"%%writefile inference.py\n\nfrom PIL import Image\nimport torch\nimport fire\n\nfrom processing_paligemma import PaliGemmaProcessor\nfrom modeling_gemma import KVCache, PaliGemmaForConditionalGeneration\nfrom utils import load_hf_model\n\n\ndef move_inputs_to_device(model_inputs: dict, device: str):\n    model_inputs = {k: v.to(device) for k, v in model_inputs.items()}\n    return model_inputs\n\n\ndef get_model_inputs(\n    processor: PaliGemmaProcessor, prompt: str, image_file_path: str, device: str\n):\n    image = Image.open(image_file_path)\n    images = [image]\n    prompts = [prompt]\n    model_inputs = processor(text=prompts, images=images)\n    model_inputs = move_inputs_to_device(model_inputs, device)\n    return model_inputs\n\n\ndef test_inference(\n    model: PaliGemmaForConditionalGeneration,\n    processor: PaliGemmaProcessor,\n    device: str,\n    prompt: str,\n    image_file_path: str,\n    max_tokens_to_generate: int,\n    temperature: float,\n    top_p: float,\n    do_sample: bool,\n):\n    model_inputs = get_model_inputs(processor, prompt, image_file_path, device)\n    input_ids = model_inputs[\"input_ids\"]\n    attention_mask = model_inputs[\"attention_mask\"]\n    pixel_values = model_inputs[\"pixel_values\"]\n\n    kv_cache = KVCache()\n\n    # Generate tokens until you see the stop token\n    stop_token = processor.tokenizer.eos_token_id\n    generated_tokens = []\n\n    for _ in range(max_tokens_to_generate):\n        # Get the model outputs\n        # TODO: remove the labels\n        outputs = model(\n            input_ids=input_ids,\n            pixel_values=pixel_values,\n            attention_mask=attention_mask,\n            kv_cache=kv_cache,\n        )\n        kv_cache = outputs[\"kv_cache\"]\n        next_token_logits = outputs[\"logits\"][:, -1, :]\n        # Sample the next token\n        if do_sample:\n            # Apply temperature\n            next_token_logits = torch.softmax(next_token_logits / temperature, dim=-1)\n            next_token = _sample_top_p(next_token_logits, top_p)\n        else:\n            next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n        assert next_token.size() == (1, 1)\n        next_token = next_token.squeeze(0)  # Remove batch dimension\n        generated_tokens.append(next_token)\n        # Stop if the stop token has been generated\n        if next_token.item() == stop_token:\n            break\n        # Append the next token to the input\n        input_ids = next_token.unsqueeze(-1)\n        attention_mask = torch.cat(\n            [attention_mask, torch.ones((1, 1), device=input_ids.device)], dim=-1\n        )\n\n    generated_tokens = torch.cat(generated_tokens, dim=-1)\n    # Decode the generated tokens\n    decoded = processor.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n\n    print(prompt + decoded)\n\n\ndef _sample_top_p(probs: torch.Tensor, p: float):\n    # (B, vocab_size)\n    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n    # (B, vocab_size)\n    probs_sum = torch.cumsum(probs_sort, dim=-1)\n    # (B, vocab_size)\n    # (Substracting \"probs_sort\" shifts the cumulative sum by 1 position to the right before masking)\n    mask = probs_sum - probs_sort > p\n    # Zero out all the probabilities of tokens that are not selected by the Top P\n    probs_sort[mask] = 0.0\n    # Redistribute the probabilities so that they sum up to 1.\n    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n    # Sample a token (its index) from the top p distribution\n    next_token = torch.multinomial(probs_sort, num_samples=1)\n    # Get the token position in the vocabulary corresponding to the sampled index\n    next_token = torch.gather(probs_idx, -1, next_token)\n    return next_token\n\n\ndef main(\n    model_path: str = None,\n    prompt: str = None,\n    image_file_path: str = None,\n    max_tokens_to_generate: int = 100,\n    temperature: float = 0.8,\n    top_p: float = 0.9,\n    do_sample: bool = False,\n    only_cpu: bool = False,\n):\n    device = \"cpu\"\n\n    if not only_cpu:\n        if torch.cuda.is_available():\n            device = \"cuda\"\n        elif torch.backends.mps.is_available():\n            device = \"mps\"\n\n    print(\"Device in use: \", device)\n\n    print(f\"Loading model\")\n    model, tokenizer = load_hf_model(model_path, device)\n    model = model.to(device).eval()\n\n    num_image_tokens = model.config.vision_config.num_image_tokens\n    image_size = model.config.vision_config.image_size\n    processor = PaliGemmaProcessor(tokenizer, num_image_tokens, image_size)\n\n    print(\"Running inference\")\n    with torch.no_grad():\n        test_inference(\n            model,\n            processor,\n            device,\n            prompt,\n            image_file_path,\n            max_tokens_to_generate,\n            temperature,\n            top_p,\n            do_sample,\n        )\n\n\nif __name__ == \"__main__\":\n    fire.Fire(main)","metadata":{"_uuid":"7fe72055-0733-4596-b1b4-a171d803ffdc","_cell_guid":"7f58ed96-89e6-49ae-b9ff-3b97fc751fde","trusted":true,"id":"M5oviTWe5aC0","executionInfo":{"status":"ok","timestamp":1730990307762,"user_tz":-330,"elapsed":5,"user":{"displayName":"","userId":""}},"outputId":"57499322-3b57-4c96-d53c-ecfe6327b964","execution":{"iopub.status.busy":"2024-11-16T12:03:22.381327Z","iopub.execute_input":"2024-11-16T12:03:22.381988Z","iopub.status.idle":"2024-11-16T12:03:22.391300Z","shell.execute_reply.started":"2024-11-16T12:03:22.381950Z","shell.execute_reply":"2024-11-16T12:03:22.390390Z"}},"outputs":[{"name":"stdout","text":"Writing inference.py\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# Launch_Inference_Script","metadata":{}},{"cell_type":"code","source":"%%writefile launch_inference.sh\n\n#!/bin/bash\n\nMODEL_PATH=\"/kaggle/working/paligemma-3b-pt-224\"\nPROMPT=\"Write the caption for this photo \"\nIMAGE_FILE_PATH=\"/kaggle/input/test-image-1/burj-khalifa-in-dubai.jpg\"\nMAX_TOKENS_TO_GENERATE=100\nTEMPERATURE=0.8\nTOP_P=0.9\nDO_SAMPLE=\"False\"\nONLY_CPU=\"False\"\n\npython inference.py \\\n    --model_path \"$MODEL_PATH\" \\\n    --prompt \"$PROMPT\" \\\n    --image_file_path \"$IMAGE_FILE_PATH\" \\\n    --max_tokens_to_generate $MAX_TOKENS_TO_GENERATE \\\n    --temperature $TEMPERATURE \\\n    --top_p $TOP_P \\\n    --do_sample $DO_SAMPLE \\\n    --only_cpu $ONLY_CPU \\\n\n","metadata":{"_uuid":"cd074813-a2e0-4296-898a-369ff9489d80","_cell_guid":"10e2319a-c60b-4481-be31-c01bf804500f","trusted":true,"collapsed":false,"id":"UqlGGvfX8qCP","executionInfo":{"status":"ok","timestamp":1730990640055,"user_tz":-330,"elapsed":14,"user":{"displayName":"","userId":""}},"outputId":"085ba14c-e461-4501-f696-9a8f0ab4da9a","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-16T12:35:19.358919Z","iopub.execute_input":"2024-11-16T12:35:19.359684Z","iopub.status.idle":"2024-11-16T12:35:19.366190Z","shell.execute_reply.started":"2024-11-16T12:35:19.359641Z","shell.execute_reply":"2024-11-16T12:35:19.365140Z"}},"outputs":[{"name":"stdout","text":"Overwriting launch_inference.sh\n","output_type":"stream"}],"execution_count":34},{"cell_type":"markdown","source":"# Run_Inference","metadata":{}},{"cell_type":"code","source":"# Make the shell script executable\n!chmod +x /kaggle/working/launch_inference.sh\n\n# Run the script\n!/kaggle/working/launch_inference.sh","metadata":{"_uuid":"18f3dc6f-edb0-4175-92fe-d996545eaaa7","_cell_guid":"a7ceda9f-7fe5-43a4-b4bc-b136e72435ba","trusted":true,"collapsed":false,"id":"hd2nLRc4-zj6","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-16T12:35:21.973413Z","iopub.execute_input":"2024-11-16T12:35:21.974100Z","iopub.status.idle":"2024-11-16T12:36:09.392418Z","shell.execute_reply.started":"2024-11-16T12:35:21.974039Z","shell.execute_reply":"2024-11-16T12:36:09.391227Z"}},"outputs":[{"name":"stdout","text":"Device in use:  cuda\nLoading model\nRunning inference\nAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\nWrite the caption for this photo the world's tallest building\n","output_type":"stream"}],"execution_count":35}]}
